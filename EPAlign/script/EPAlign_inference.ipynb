{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "from transformers import Wav2Vec2Processor\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PROJECT_PATH = os.path.join('/', *os.getcwd().split(os.sep)[:-2])\n",
    "# EMOTION_FEATURES_SAVE_PATH, the path to save the extracted features, e.g. EPAlign/mmefeature/tmp\n",
    "EMOTION_FEATURES_SAVE_PATH = os.path.join(PROJECT_PATH, 'EPAlign', 'mmefeature', 'tmp')\n",
    "os.makedirs(EMOTION_FEATURES_SAVE_PATH, exist_ok=True)\n",
    "# PRETRAIN_MODEL is the pretrained model name, e.g. ViT-B/32\n",
    "PRETRAIN_MODEL = \"ViT-B/32\"\n",
    "# PRETRAIN_WAV2VEC2_PATH is the pretrained model path, e.g. EPAlign/ckpt/base/wav2vec2\n",
    "PRETRAIN_WAV2VEC2_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base/wav2vec2\"\n",
    "# PRETRAIN_MODEL_PATH is the pretrained model path, e.g. EPAlign/ckpt/base\n",
    "PRETRAIN_MODEL_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base\"\n",
    "# PROCESSED_WAV2VEC2_PATH is the path to the Wav2Vec2Processor\n",
    "PROCESSED_WAV2VEC2_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base/wav2vec2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explict point out speech emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotiontags, probable emotions, e.g. \"angry\", \"happy\", \"neutral\", \"sad\", \"surprise\" (appear in ESD dataset)\n",
    "emotiontags = [\"angry\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "\n",
    "DATASET = \"ESD\"\n",
    "# FINETUNE_MODEL is the finetuned model path, e.g. EPAlign/ckpt/ESD/best_model.pt\n",
    "FINETUNE_MODEL = f\"{PROJECT_PATH}/EPAlign/ckpt/{DATASET}/best_model.pt\"\n",
    "\n",
    "model, preprocess = clip.load(PRETRAIN_MODEL, device=device, jit=False, download_root=PRETRAIN_MODEL_PATH)\n",
    "model.load_state_dict(torch.load(FINETUNE_MODEL))\n",
    "\n",
    "test_prompts = [f'A person speaking with a feeling of {emo}' for emo in emotiontags]\n",
    "emo_prompt = clip.tokenize(test_prompts).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    emo_prompt_features = model.encode_text(emo_prompt)\n",
    "    emo_prompt_features /= emo_prompt_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "emo_prompt_features = emo_prompt_features.split(1, dim=0)\n",
    "\n",
    "os.makedirs(f'{EMOTION_FEATURES_SAVE_PATH}/explict', exist_ok=True)\n",
    "for i in range(len(emo_prompt_features)):\n",
    "    torch.save(emo_prompt_features[i], f\"{EMOTION_FEATURES_SAVE_PATH}/explict/{emotiontags[i]}.pt\")\n",
    "    print(f\"Save {emotiontags[i]} feature in {EMOTION_FEATURES_SAVE_PATH}/explict/{emotiontags[i]}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implict point out speech emotion (with Audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotiontags, probable emotions, e.g. \"angry\", \"happy\", \"neutral\", \"sad\", \"surprise\" (appear in ESD dataset)\n",
    "emotiontags = [\"angry\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "# test_wav_path is the path to the wav file, e.g. test/wav/test_audio.wav\n",
    "test_wav_path = f\"{PROJECT_PATH}/EPAlign/test/wav/test_audio.wav\"\n",
    "\n",
    "DATASET = \"ESD\"\n",
    "# FINETUNE_MODEL is the finetuned model path, e.g. EPAlign/ckpt/ESD/best_model.pt\n",
    "FINETUNE_MODEL = f\"{PROJECT_PATH}/EPAlign/ckpt/{DATASET}/best_model_proj_logit.pt\"\n",
    "test_prompts = [f'A person speaking with a feeling of {emo}' for emo in emotiontags]\n",
    "\n",
    "class CLAP(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config, prompt_pretrain_model, prompt_pretrain_model_path):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.init_weights()\n",
    "        width = 1024\n",
    "        scale = width ** -0.5\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, 512))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.prompt_model, self.prompt_processor = clip.load(prompt_pretrain_model, jit=False, download_root=prompt_pretrain_model_path)\n",
    "        self.prompt_model.to(device)\n",
    "    def forward(self, wavs, prompts):\n",
    "        audio_features = torch.tensor([]).to(device)\n",
    "        for wav in wavs:\n",
    "            audio_feature = self.wav2vec2(wav)\n",
    "            audio_feature = audio_feature[0]\n",
    "            audio_feature = torch.mean(audio_feature, dim=1)\n",
    "            audio_features = torch.cat((audio_features, audio_feature), dim=0)\n",
    "        audio_features = audio_features @ self.proj\n",
    "\n",
    "        prompt_features = clip.tokenize(prompts).to(device)\n",
    "        prompt_features = self.prompt_model.encode_text(prompt_features)\n",
    "        # normalized features\n",
    "        audio_features = audio_features / audio_features.norm(dim=1, keepdim=True)\n",
    "        prompt_features = prompt_features / prompt_features.norm(dim=1, keepdim=True)\n",
    "        audio_features = audio_features.float()\n",
    "        prompt_features = prompt_features.float()\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp().float()\n",
    "        logits_per_audio = logit_scale * audio_features @ prompt_features.t()\n",
    "        logits_per_text = logits_per_audio.t()\n",
    "        return logits_per_audio, logits_per_text\n",
    "\n",
    "model = CLAP.from_pretrained(PRETRAIN_WAV2VEC2_PATH, prompt_pretrain_model=PRETRAIN_MODEL, prompt_pretrain_model_path=PRETRAIN_MODEL_PATH).to(device)\n",
    "model.load_state_dict(torch.load(FINETUNE_MODEL))\n",
    "\n",
    "\n",
    "test_wav, _ = librosa.load(test_wav_path, sr=16000)\n",
    "# audio = torch.from_numpy(wav).float()\n",
    "audio_processor = Wav2Vec2Processor.from_pretrained(PROCESSED_WAV2VEC2_PATH)\n",
    "test_audio = audio_processor(test_wav, sampling_rate=16000)\n",
    "test_audio = test_audio[\"input_values\"][0]\n",
    "test_audio = test_audio.reshape(1, -1)\n",
    "test_audio = torch.from_numpy(test_audio).to(device).float()\n",
    "with torch.no_grad():\n",
    "    logits_per_audio, _ = model(test_audio.unsqueeze(0), test_prompts)\n",
    "    probs = logits_per_audio.softmax(dim=-1).cpu().numpy()\n",
    "    print(f\"Predicted emotion: {emotiontags[np.argmax(probs)]}\")\n",
    "    os.makedirs(f'{EMOTION_FEATURES_SAVE_PATH}/implict_audio', exist_ok=True)\n",
    "    torch.save(emo_prompt_features[np.argmax(probs)].squeeze(), f\"{EMOTION_FEATURES_SAVE_PATH}/implict_audio/{test_wav_path.split(\"/\")[-1][:-4]}.pt\")\n",
    "    print(f\"Save {emotiontags[np.argmax(probs)]} feature in {EMOTION_FEATURES_SAVE_PATH}/implict_audio/{test_wav_path.split(\"/\")[-1][:-4]}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implict point out speech emotion (with Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotiontags, probable emotions, e.g. \"Surprise\", \"Fear\", \"Disgust\", \"Happiness\", \"Sadness\", \"Anger\", \"Neutral\"  (appear in RAF-DB dataset)\n",
    "emotiontags = [\"Surprise\", \"Fear\", \"Disgust\", \"Happiness\", \"Sadness\", \"Anger\", \"Neutral\"]\n",
    "test_prompts = [f'A person speaking with a feeling of {emo}' for emo in emotiontags]\n",
    "# test_img_path is the path to the img file, e.g. test/img/test_img.jpg\n",
    "test_img_path = f\"{PROJECT_PATH}/EPAlign/test/img/test_img.jpg\"\n",
    "\n",
    "DATASET = \"RAF\"\n",
    "# FINETUNE_MODEL is the finetuned model path, e.g. EPAlign/ckpt/RAF/RAF_ft.pt\n",
    "FINETUNE_MODEL = f\"{PROJECT_PATH}/EPAlign/ckpt/{DATASET}/RAF_ft.pt\"\n",
    "model, preprocess_img = clip.load(PRETRAIN_MODEL, device=device, jit=False, download_root=PRETRAIN_MODEL_PATH)\n",
    "model.load_state_dict(torch.load(FINETUNE_MODEL))\n",
    "with torch.no_grad():\n",
    "    img = Image.open(test_img_path)\n",
    "    logits_per_image, _ = model(preprocess_img(img).unsqueeze(0).to(device), clip.tokenize(test_prompts).to(device))\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    print(f\"Predicted emotion: {emotiontags[np.argmax(probs)]}\")\n",
    "    os.makedirs(f'{EMOTION_FEATURES_SAVE_PATH}/implict_img', exist_ok=True)\n",
    "    torch.save(emo_prompt_features[np.argmax(probs)].squeeze(), f\"{EMOTION_FEATURES_SAVE_PATH}/implict_img/{test_img_path.split(\"/\")[-1][:-4]}.pt\")\n",
    "    print(f\"Save {emotiontags[np.argmax(probs)]} feature in {EMOTION_FEATURES_SAVE_PATH}/implict_img/{test_img_path.split(\"/\")[-1][:-4]}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implict point out speech emotion (with Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotiontags, probable emotions, e.g. \"neutral\", \"joy\", \"sad\", \"angry\", \"surprise\", \"fearful\", \"disgust\" (appear in MELD dataset)\n",
    "emotiontags = [\"neutral\", \"joy\", \"sad\", \"angry\", \"surprise\", \"fearful\", \"disgust\"]\n",
    "test_prompts = [f'A person speaking with a feeling of {emo}' for emo in emotiontags]\n",
    "# test_text_f_path is the path to the text feature file, e.g. test/img/test_text_f.pt\n",
    "test_text_f_path = f\"{PROJECT_PATH}/EPAlign/test/text_f/test_text_f.pt\"\n",
    "\n",
    "DATASET = \"MELD\"\n",
    "# FINETUNE_MODEL is the finetuned model path, e.g. EPAlign/ckpt/MELD/MELD_text_ft.pt\n",
    "FINETUNE_MODEL = f\"{PROJECT_PATH}/EPAlign/ckpt/{DATASET}/MELD_text_ft.pt\"\n",
    "\n",
    "class Concat_text_Model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_label_feature_dim=512,\n",
    "                 input_text_feature_dim=4096,\n",
    "                 fused_dim=512,\n",
    "                 num_heads=8,\n",
    "                 ):\n",
    "        super(Concat_text_Model, self).__init__()\n",
    "        self.input_label_feature_dim = input_label_feature_dim\n",
    "        self.input_text_feature_dim = input_text_feature_dim\n",
    "\n",
    "        self.fused_dim = fused_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        # self.text_linear = nn.Linear(self.input_text_feature_dim, self.fused_dim)\n",
    "        # self.atten = nn.MultiheadAttention(3 * self.fused_dim, self.num_heads)\n",
    "        scale = self.fused_dim ** -0.5\n",
    "        self.fuse_proj = nn.Parameter(scale * torch.randn(self.input_text_feature_dim, self.fused_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.label_linear = nn.Linear(self.input_label_feature_dim, self.fused_dim)\n",
    "\n",
    "    def forward(self, text_features, label_features):\n",
    "        # text_features: (batch_size, seq_len, input_text_feature_dim) seq_len = 1 e.g.\n",
    "        label_features = self.label_linear(label_features)\n",
    "        text_features = text_features @ self.fuse_proj\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "        label_features = label_features / label_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features.float()\n",
    "        label_features = label_features.float()\n",
    "        logit_scale = self.logit_scale.exp().float()\n",
    "        logits_per_fused = logit_scale * text_features @ label_features.t()\n",
    "        logits_per_label = logits_per_fused.t()\n",
    "\n",
    "        return logits_per_fused, logits_per_label\n",
    "    \n",
    "    def extract_text_feature(self, text_features):\n",
    "        # text_features = self.text_linear(text_features)\n",
    "        text_features = text_features @ self.fuse_proj\n",
    "        return text_features\n",
    "\n",
    "model = Concat_text_Model().to(device)\n",
    "model.load_state_dict(torch.load(FINETUNE_MODEL))\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_feature = torch.load(test_text_f_path)\n",
    "    text_feature = text_feature.unsqueeze(0)\n",
    "    logits_per_fused, _ = model(text_feature.to(device), clip.tokenize(test_prompts).to(device))\n",
    "    probs = logits_per_fused.softmax(dim=-1).cpu().numpy()\n",
    "    print(f\"Predicted emotion: {emotiontags[np.argmax(probs)]}\")\n",
    "    os.makedirs(f'{EMOTION_FEATURES_SAVE_PATH}/implict_text', exist_ok=True)\n",
    "    torch.save(emo_prompt_features[np.argmax(probs)].squeeze(), f\"{EMOTION_FEATURES_SAVE_PATH}/implict_text/{test_text_f_path.split(\"/\")[-1]}\")\n",
    "    print(f\"Save {emotiontags[np.argmax(probs)]} feature in {EMOTION_FEATURES_SAVE_PATH}/implict_text/{test_text_f_path.split(\"/\")[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implict point out speech emotion (with Text & Video & Audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotiontags, probable emotions, e.g. \"neutral\", \"joy\", \"sad\", \"angry\", \"surprise\", \"fearful\", \"disgust\" (appear in MELD dataset)\n",
    "emotiontags = [\"neutral\", \"joy\", \"sad\", \"angry\", \"surprise\", \"fearful\", \"disgust\"]\n",
    "test_prompts = [f'A person speaking with a feeling of {emo}' for emo in emotiontags]\n",
    "# test_text_f_path is the path to the text feature file, e.g. test/img/test_text_f.pt\n",
    "test_text_f_path = f\"{PROJECT_PATH}/EPAlign/test/text_f/test_text_f.pt\"\n",
    "test_visual_f_path = f\"{PROJECT_PATH}/EPAlign/test/visual_f/test_visual_f.pt\"\n",
    "test_audio_f_path = f\"{PROJECT_PATH}/EPAlign/test/audio_f/test_audio_f.pt\"\n",
    "\n",
    "DATASET = \"MELD\"\n",
    "# FINETUNE_MODEL is the finetuned model path, e.g. EPAlign/ckpt/MELD/MELD_fuse_ft.pt\n",
    "FINETUNE_MODEL = f\"{PROJECT_PATH}/EPAlign/ckpt/{DATASET}/MELD_fuse_ft.pt\"\n",
    "\n",
    "class Concat_SelfAttention_Model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_text_feature_dim=4096,\n",
    "                 input_visual_feature_dim=512,\n",
    "                 input_audio_feature_dim=512,\n",
    "                 fused_dim=512,\n",
    "                 is_prompt_linear=False,\n",
    "                 is_text_linear=True,\n",
    "                 is_visual_linear=False,\n",
    "                 is_audio_linear=False,\n",
    "                 num_heads=8,\n",
    "                 prompt_pretrain_model=\"\",\n",
    "                 prompt_pretrain_model_path=\"\"\n",
    "                 ):\n",
    "        super(Concat_SelfAttention_Model, self).__init__()\n",
    "        self.input_text_feature_dim = input_text_feature_dim\n",
    "        self.input_visual_feature_dim = input_visual_feature_dim\n",
    "        self.input_audio_feature_dim = input_audio_feature_dim\n",
    "        self.fused_dim = fused_dim\n",
    "        self.is_prompt_linear = is_prompt_linear\n",
    "        self.is_text_linear = is_text_linear\n",
    "        self.is_visual_linear = is_visual_linear\n",
    "        self.is_audio_linear = is_audio_linear\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        if self.is_text_linear:\n",
    "            self.text_linear = nn.Linear(self.input_text_feature_dim, self.fused_dim)\n",
    "        if self.is_visual_linear:\n",
    "            self.visual_linear = nn.Linear(self.input_visual_feature_dim, self.fused_dim)\n",
    "        if self.is_audio_linear:\n",
    "            self.audio_linear = nn.Linear(self.input_audio_feature_dim, self.fused_dim)\n",
    "        \n",
    "        self.atten = nn.MultiheadAttention(3 * self.fused_dim, self.num_heads)\n",
    "\n",
    "        scale = (3 * self.fused_dim) ** -0.5\n",
    "        self.fuse_proj = nn.Parameter(scale * torch.randn(3 * self.fused_dim, self.fused_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        \n",
    "        self.prompt_model, self.prompt_processor = clip.load(prompt_pretrain_model, jit=False, download_root=prompt_pretrain_model_path)\n",
    "        self.prompt_model.to(device)\n",
    "        if self.is_prompt_linear:\n",
    "            self.prompt_linear = nn.Linear(512, self.fused_dim)\n",
    "\n",
    "    def forward(self, text_features, visual_features, audio_features, prompts):\n",
    "        # text_features: (batch_size, seq_len, input_text_feature_dim) seq_len = 1 e.g.\n",
    "        if self.is_text_linear:\n",
    "            text_features = self.text_linear(text_features)\n",
    "        if self.is_visual_linear:\n",
    "            visual_features = self.visual_linear(visual_features)\n",
    "        if self.is_audio_linear:\n",
    "            audio_features = self.audio_linear(audio_features)\n",
    "        prompt_features = clip.tokenize(prompts).to(device)\n",
    "        prompt_features = self.prompt_model.encode_text(prompt_features)\n",
    "        if self.is_prompt_linear:\n",
    "            prompt_features = self.prompt_linear(prompt_features)\n",
    "\n",
    "        x = torch.cat([text_features, visual_features, audio_features], dim=-1)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, _ = self.atten(x, x, x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        x = x.squeeze(1)\n",
    "        fused_features = x @ self.fuse_proj\n",
    "\n",
    "        fused_features = fused_features / fused_features.norm(dim=1, keepdim=True)\n",
    "        prompt_features = prompt_features / prompt_features.norm(dim=1, keepdim=True)\n",
    "        fused_features = fused_features.float()\n",
    "        prompt_features = prompt_features.float()\n",
    "\n",
    "        logit_scale = self.logit_scale.exp().float()\n",
    "        logits_per_fused = logit_scale * fused_features @ prompt_features.t()\n",
    "        logits_per_label = logits_per_fused.t()\n",
    "\n",
    "        return logits_per_fused, logits_per_label\n",
    "    \n",
    "    def extract_fused_feature(self, text_features, visual_features, audio_features):\n",
    "        if self.is_text_linear:\n",
    "            text_features = self.text_linear(text_features)\n",
    "        if self.is_visual_linear:\n",
    "            visual_features = self.visual_linear(visual_features)\n",
    "        if self.is_audio_linear:\n",
    "            audio_features = self.audio_linear(audio_features)\n",
    "\n",
    "        x = torch.cat([text_features, visual_features, audio_features], dim=-1)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, _ = self.atten(x, x, x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        x = x.squeeze(1)\n",
    "        fused_features = x @ self.fuse_proj\n",
    "\n",
    "        return fused_features\n",
    "\n",
    "model = Concat_SelfAttention_Model(prompt_pretrain_model=PRETRAIN_MODEL, prompt_pretrain_model_path=PRETRAIN_MODEL_PATH).to(device)\n",
    "model.load_state_dict(torch.load(FINETUNE_MODEL))\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_feature = torch.load(test_text_f_path)\n",
    "    visual_feature = torch.load(test_visual_f_path)\n",
    "    audio_feature = torch.load(test_audio_f_path)\n",
    "    text_feature = text_feature.unsqueeze(0)\n",
    "    visual_feature = visual_feature.unsqueeze(0)\n",
    "    audio_feature = audio_feature.unsqueeze(0)\n",
    "    logits_per_fused, _ = model(text_feature.to(device), visual_feature.to(device), audio_feature.to(device), clip.tokenize(test_prompts).to(device))\n",
    "    probs = logits_per_fused.softmax(dim=-1).cpu().numpy()\n",
    "    print(f\"Predicted emotion: {emotiontags[np.argmax(probs)]}\")\n",
    "    os.makedirs(f'{EMOTION_FEATURES_SAVE_PATH}/implict_fused', exist_ok=True)\n",
    "    torch.save(emo_prompt_features[np.argmax(probs)].squeeze(), f\"{EMOTION_FEATURES_SAVE_PATH}/implict_fused/{test_text_f_path.split(\"/\")[-1]}\")\n",
    "    print(f\"Save {emotiontags[np.argmax(probs)]} feature in {EMOTION_FEATURES_SAVE_PATH}/implict_fused/{test_text_f_path.split(\"/\")[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMTTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
