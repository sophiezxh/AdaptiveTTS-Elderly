{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPAlign Prompt and Fused-feature (text, audio, vision) Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import clip\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import logging\n",
    "\n",
    "# DATASET is the dataset name model trained on, e.g. MELD\n",
    "DATASET = \"MELD\"\n",
    "\n",
    "# BATCH_SIZE should smaller/equal to the category of the emotion, e.g. for MELD, the category is 6\n",
    "BATCH_SIZE = 6\n",
    "EPOCH = 100\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PROJECT_PATH = os.path.join('/', *os.getcwd().split(os.sep)[:-2])\n",
    "# MELD processed path\n",
    "PROCESSED_MELD_PATH = f\"{PROJECT_PATH}/data/meld/processed\"\n",
    "# PRETRAIN_CLIP_MODEL is the pretrained CLIP model, e.g. ViT-B-32\n",
    "PRETRAIN_CLIP_MODEL = \"ViT-B/32\"\n",
    "# PRETRAIN_CLIP_MODEL_PATH is the pretrained model path, e.g. EPAlign/ckpt/base\n",
    "PRETRAIN_CLIP_MODEL_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base\"\n",
    "# LOG_PATH is the log path, e.g. EPAlign/log\n",
    "LOG_PATH = f\"{PROJECT_PATH}/EPAlign/log\"\n",
    "# CKPT_PATH is the path to save checkpoint, e.g. EPAlign/ckpt/ESD\n",
    "CKPT_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/{DATASET}_fused\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuse Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concat_SelfAttention_Model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_text_feature_dim=4096,\n",
    "                 input_visual_feature_dim=512,\n",
    "                 input_audio_feature_dim=512,\n",
    "                 fused_dim=512,\n",
    "                 is_prompt_linear=False,\n",
    "                 is_text_linear=True,\n",
    "                 is_visual_linear=False,\n",
    "                 is_audio_linear=False,\n",
    "                 num_heads=8,\n",
    "                 prompt_pretrain_model=\"\",\n",
    "                 prompt_pretrain_model_path=\"\"\n",
    "                 ):\n",
    "        super(Concat_SelfAttention_Model, self).__init__()\n",
    "        self.input_text_feature_dim = input_text_feature_dim\n",
    "        self.input_visual_feature_dim = input_visual_feature_dim\n",
    "        self.input_audio_feature_dim = input_audio_feature_dim\n",
    "        self.fused_dim = fused_dim\n",
    "        self.is_prompt_linear = is_prompt_linear\n",
    "        self.is_text_linear = is_text_linear\n",
    "        self.is_visual_linear = is_visual_linear\n",
    "        self.is_audio_linear = is_audio_linear\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        if self.is_text_linear:\n",
    "            self.text_linear = nn.Linear(self.input_text_feature_dim, self.fused_dim)\n",
    "        if self.is_visual_linear:\n",
    "            self.visual_linear = nn.Linear(self.input_visual_feature_dim, self.fused_dim)\n",
    "        if self.is_audio_linear:\n",
    "            self.audio_linear = nn.Linear(self.input_audio_feature_dim, self.fused_dim)\n",
    "        \n",
    "        self.atten = nn.MultiheadAttention(3 * self.fused_dim, self.num_heads)\n",
    "\n",
    "        scale = (3 * self.fused_dim) ** -0.5\n",
    "        self.fuse_proj = nn.Parameter(scale * torch.randn(3 * self.fused_dim, self.fused_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        \n",
    "        self.prompt_model, self.prompt_processor = clip.load(prompt_pretrain_model, jit=False, download_root=prompt_pretrain_model_path)\n",
    "        self.prompt_model.to(device)\n",
    "        if self.is_prompt_linear:\n",
    "            self.prompt_linear = nn.Linear(512, self.fused_dim)\n",
    "\n",
    "    def forward(self, text_features, visual_features, audio_features, prompts):\n",
    "        # text_features: (batch_size, seq_len, input_text_feature_dim) seq_len = 1 e.g.\n",
    "        if self.is_text_linear:\n",
    "            text_features = self.text_linear(text_features)\n",
    "        if self.is_visual_linear:\n",
    "            visual_features = self.visual_linear(visual_features)\n",
    "        if self.is_audio_linear:\n",
    "            audio_features = self.audio_linear(audio_features)\n",
    "        prompt_features = clip.tokenize(prompts).to(device)\n",
    "        prompt_features = self.prompt_model.encode_text(prompt_features)\n",
    "        if self.is_prompt_linear:\n",
    "            prompt_features = self.prompt_linear(prompt_features)\n",
    "\n",
    "        x = torch.cat([text_features, visual_features, audio_features], dim=-1)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, _ = self.atten(x, x, x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        x = x.squeeze(1)\n",
    "        fused_features = x @ self.fuse_proj\n",
    "\n",
    "        fused_features = fused_features / fused_features.norm(dim=1, keepdim=True)\n",
    "        prompt_features = prompt_features / prompt_features.norm(dim=1, keepdim=True)\n",
    "        fused_features = fused_features.float()\n",
    "        prompt_features = prompt_features.float()\n",
    "\n",
    "        logit_scale = self.logit_scale.exp().float()\n",
    "        logits_per_fused = logit_scale * fused_features @ prompt_features.t()\n",
    "        logits_per_label = logits_per_fused.t()\n",
    "\n",
    "        return logits_per_fused, logits_per_label\n",
    "    \n",
    "    def extract_fused_feature(self, text_features, visual_features, audio_features):\n",
    "        if self.is_text_linear:\n",
    "            text_features = self.text_linear(text_features)\n",
    "        if self.is_visual_linear:\n",
    "            visual_features = self.visual_linear(visual_features)\n",
    "        if self.is_audio_linear:\n",
    "            audio_features = self.audio_linear(audio_features)\n",
    "\n",
    "        x = torch.cat([text_features, visual_features, audio_features], dim=-1)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, _ = self.atten(x, x, x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        x = x.squeeze(1)\n",
    "        fused_features = x @ self.fuse_proj\n",
    "\n",
    "        return fused_features\n",
    "model = Concat_SelfAttention_Model(prompt_pretrain_model=PRETRAIN_CLIP_MODEL, prompt_pretrain_model_path=PRETRAIN_CLIP_MODEL_PATH).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MELD Multimodal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MELD_Multimodal_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 mode=\"train\",\n",
    "                 filelist_path=\"EMITTS/filelist\",\n",
    "                 feature_path = \"data/meld/processed\",\n",
    "                 is_with_unnomal_sampels=False,\n",
    "                 is_return_sample_name=False):\n",
    "        self.mode = mode\n",
    "        self.datalist_path = f'{filelist_path}/meld_name_emotion_{self.mode}_filelist.txt'\n",
    "        self.text_feature_path = f'{feature_path}/text_feature/{self.mode}_Dialogue_ID_Utterance_ID'\n",
    "        self.visual_feature_path = f'{feature_path}/visual_feature/{self.mode}_Dialogue_ID_Utterance_ID'\n",
    "        self.audio_feature_path = f'{feature_path}/wav_feature/{self.mode}_Dialogue_ID_Utterance_ID'\n",
    "        self.is_with_unnomal_sampels = is_with_unnomal_sampels\n",
    "        self.is_return_sample_name = is_return_sample_name\n",
    "        self.data = self.load_data()\n",
    "        self.label2text = {\n",
    "            '1': \"neutral\",\n",
    "            '2': \"joy\",\n",
    "            '3': \"sad\",\n",
    "            '4': \"angry\",\n",
    "            '5': \"surprise\",\n",
    "            '6': \"fearful\",\n",
    "            '7': \"disgust\",\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = data[1]\n",
    "        emotiontag = self.label2text[label]\n",
    "        prompt = f\"A person speaking with a feeling of {emotiontag}\"\n",
    "        text_feature = torch.load(f'{self.text_feature_path}/{data[0]}.pt', map_location='cpu')\n",
    "        visual_feature = torch.load(f'{self.visual_feature_path}/{data[0]}.pt', map_location='cpu')\n",
    "        audio_feature = torch.load(f'{self.audio_feature_path}/{data[0]}.pt', map_location='cpu')\n",
    "        if self.is_return_sample_name:\n",
    "            return text_feature, visual_feature, audio_feature, prompt, int(label), data[0]\n",
    "        return text_feature, visual_feature, audio_feature, prompt, int(label)\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.datalist_path, encoding='utf-8') as f:\n",
    "            data = [line.strip().split(\"|\") for line in f]\n",
    "        if not self.is_with_unnomal_sampels:\n",
    "            if self.mode == 'train':\n",
    "                # del 'dia125_utt3' in train\n",
    "                data = [d for d in data if d[0] != 'dia125_utt3']\n",
    "            elif self.mode == 'val':\n",
    "                # del 'dia110_utt7' in val\n",
    "                data = [d for d in data if d[0] != 'dia110_utt7']\n",
    "            elif self.mode == 'test':\n",
    "                # del 'dia110_utt7' in test\n",
    "                data = [d for d in data if d[0] != 'dia38_utt4']\n",
    "        return data\n",
    "    \n",
    "train_dataset = MELD_Multimodal_Dataset(mode=\"train\", feature_path=PROCESSED_MELD_PATH)\n",
    "val_dataset = MELD_Multimodal_Dataset(mode=\"val\", feature_path=PROCESSED_MELD_PATH)\n",
    "test_dataset = MELD_Multimodal_Dataset(mode=\"test\", feature_path=PROCESSED_MELD_PATH)\n",
    "# (9988, 1109, 2609)\n",
    "assert len(train_dataset) == 9988\n",
    "assert len(val_dataset) == 1109\n",
    "assert len(test_dataset) == 2609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Batch Sample (ensures no same class per batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler, DataLoader\n",
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    BatchSampler - from a MELD-Multimodal-like dataset, samples n_classes and within these classes samples n_samples.\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text_features = [sample[0] for sample in batch]\n",
    "    text_features = torch.stack(text_features).to(device)\n",
    "    visual_features = [sample[1] for sample in batch]\n",
    "    visual_features = torch.stack(visual_features).to(device)\n",
    "    audio_features = [sample[2] for sample in batch]\n",
    "    audio_features = torch.stack(audio_features).to(device)\n",
    "    prompts = [sample[3] for sample in batch]\n",
    "    # prompts = torch.stack(prompts).to(device)\n",
    "    labels = [sample[4] for sample in batch]\n",
    "    labels = torch.tensor(labels).to(device)\n",
    "    return text_features, visual_features, audio_features, prompts, labels\n",
    "\n",
    "train_labels = torch.tensor([item[4] for item in train_dataset])\n",
    "train_sampler = BalancedBatchSampler(train_labels, BATCH_SIZE, 1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler, collate_fn=collate_fn)\n",
    "\n",
    "dev_labels = torch.tensor([item[4] for item in val_dataset])\n",
    "dev_sampler = BalancedBatchSampler(dev_labels, BATCH_SIZE, 1)\n",
    "dev_dataloader = DataLoader(val_dataset, batch_sampler=dev_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "loss_fused = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "import itertools\n",
    "parameters = itertools.chain(model.text_linear.parameters(), [model.fuse_proj, model.logit_scale])\n",
    "optimizer = optim.Adam(parameters, lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "file_handler = logging.FileHandler(f\"{LOG_PATH}/log_prompt_audio_{DATASET}.txt\")\n",
    "\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "log = logging.getLogger('')\n",
    "log.addHandler(file_handler)\n",
    "log.info('finetune start...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_dev_loss = 1e5\n",
    "best_epoch = -1\n",
    "for epoch in range(EPOCH):\n",
    "    logging.info((f\"running epoch {epoch}, best test loss {best_dev_loss} after epoch {best_epoch}\"))\n",
    "    step = 0\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    train_pbar = tqdm(train_dataloader, leave=False)\n",
    "    for batch in train_pbar:\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        text_features, visual_features, audio_features, prompts, labels = batch\n",
    "        # change features type\n",
    "        text_features = text_features.float()\n",
    "        visual_features = visual_features.float()\n",
    "        audio_features = audio_features.float()\n",
    "\n",
    "        logits_per_fused, logits_per_label = model(text_features, visual_features, audio_features, prompts)\n",
    "        ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "        total_loss = (loss_fused(logits_per_fused, ground_truth) + loss_fused(logits_per_label, ground_truth)) / 2\n",
    "        total_loss.backward()\n",
    "        train_loss += total_loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n",
    "    train_loss /= step\n",
    "\n",
    "    step = 0\n",
    "    dev_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        dev_pbar = tqdm(dev_dataloader, leave=False)\n",
    "        for batch in dev_pbar:\n",
    "            step += 1\n",
    "            text_features, visual_features, audio_features, prompts, labels = batch\n",
    "            # change features type\n",
    "            text_features = text_features.float()\n",
    "            visual_features = visual_features.float()\n",
    "            audio_features = audio_features.float()\n",
    "            logits_per_fused, logits_per_label = model(text_features, visual_features, audio_features, prompts)\n",
    "            ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "            total_loss = (loss_fused(logits_per_fused, ground_truth) + loss_fused(logits_per_label, ground_truth)) / 2\n",
    "            dev_loss += total_loss.item()\n",
    "            dev_pbar.set_description(f\"dev batchCE: {total_loss.item()}\", refresh=True)\n",
    "        if dev_loss < best_dev_loss:\n",
    "            best_dev_loss = dev_loss\n",
    "            best_epoch = epoch\n",
    "        dev_loss /= step\n",
    "        torch.save(model, f\"{CKPT_PATH}/best_model.pt\")\n",
    "    logging.info(f\"epoch {epoch}, train loss {train_loss}, dev loss {dev_loss}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMTTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
