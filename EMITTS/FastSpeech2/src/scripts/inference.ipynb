{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point out model in config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "\n",
    "import torch\n",
    "from lightning import seed_everything\n",
    "from config.config import TrainConfig\n",
    "from src.models import Generator, TorchSTFT\n",
    "from src.models.acoustic_model.fastspeech.lightning_model import FastSpeechLightning\n",
    "from src.utils.utils import set_up_logger, write_wav, crash_with_msg\n",
    "from src.utils.vocoder_utils import load_checkpoint, synthesize_wav_from_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datalist):\n",
    "    with open(datalist, encoding='utf-8') as f:\n",
    "        data = [line.strip().split(\"|\") for line in f]\n",
    "    return data\n",
    "\n",
    "all_data = load_data('path/to/all.txt')\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(all_data, columns=['x','y','z', 'phone', 'text'])\n",
    "text2phone = dict()\n",
    "for i, row in df.iterrows():\n",
    "    text = row['text'].lower()\n",
    "    phone = row['phone']\n",
    "    if not text in text2phone.keys():\n",
    "        text2phone[text] = phone[1:-1]\n",
    "    # print(text2phone)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text2phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class ESDDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 datalist=\"path/to/test/filelist\"):\n",
    "        self.datalist = datalist\n",
    "        self.data = self.load_data()\n",
    "        self.text2label = {\n",
    "            \"neutral\": 0,\n",
    "            \"angry\": 1,\n",
    "            \"happy\": 2,\n",
    "            \"sad\": 3,\n",
    "            \"surprise\": 4,\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_name = self.data[idx][0].split('/')[-1][:-4]\n",
    "        speaker_id = int(self.data[idx][1])\n",
    "        phone = self.data[idx][4]\n",
    "        emotion = self.data[idx][3]\n",
    "        emotion_id = self.text2label[emotion]\n",
    "        return audio_name, speaker_id, phone, emotion_id\n",
    "    \n",
    "    def load_data(self):\n",
    "        with open(self.datalist, encoding='utf-8') as f:\n",
    "            data = [line.strip().split(\"|\") for line in f]\n",
    "        return data\n",
    "# train_dataset = ESDDataset(preprocess=processor_CLAP)\n",
    "test_dataset = ESDDataset(datalist='path/to/test/filelist')\n",
    "# len(train_dataset), len(test_dataset), train_dataset[0][0].shape, train_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset), test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dict(config, phone_sequence, speaker_id, emotion_id):\n",
    "    with open(config.phones_path, \"r\") as f:\n",
    "        phones_mapping = json.load(f)\n",
    "    phone_ids = []\n",
    "    for p in phone_sequence.split(\" \"):\n",
    "        try:\n",
    "            phone_ids.append(phones_mapping[p])\n",
    "        except KeyError:\n",
    "            crash_with_msg(\n",
    "                f\"Couldn't map input sequence: {phone_sequence} into phone ids. \\n\"\n",
    "                f\"Supported phones: {phones_mapping} \\n\"\n",
    "                f\"Phone: {p} is not in a dictionary.\"\n",
    "            )\n",
    "    texts = torch.tensor(phone_ids).long().unsqueeze(0)\n",
    "    text_lens = torch.tensor([texts.shape[1]]).long()\n",
    "    ids = [f\"{speaker_id}_0_{emotion_id}\"]\n",
    "    speakers = torch.tensor([speaker_id])\n",
    "    emotions = torch.tensor([emotion_id])\n",
    "    mels, mel_lens, pitches, energies, durations, egemap_features = [None] * 6\n",
    "    batch_dict = {\n",
    "        \"ids\": ids,\n",
    "        \"speakers\": speakers,\n",
    "        \"emotions\": emotions,\n",
    "        \"texts\": texts,\n",
    "        \"text_lens\": text_lens,\n",
    "        \"mels\": mels,\n",
    "        \"mel_lens\": mel_lens,\n",
    "        \"pitches\": pitches,\n",
    "        \"energies\": energies,\n",
    "        \"durations\": durations,\n",
    "        \"egemap_features\": egemap_features,\n",
    "    }\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "set_up_logger(\"inference.log\")\n",
    "config = TrainConfig()\n",
    "\n",
    "# phone_sequence = \"S P IY2 K ER1 F AY1 V  T AO1 K IH0 NG W IH0 TH AE1 NG G R IY0 IH0 M OW0 SH AH0 N\"\n",
    "# speaker_id = 5\n",
    "# emotion_id = 1\n",
    "# generated_audio_path = \"path/to/test.wav\"\n",
    "\n",
    "seed_everything(config.seed)\n",
    "vocoder = Generator(**asdict(config))\n",
    "stft = TorchSTFT(**asdict(config))\n",
    "vocoder_state_dict = load_checkpoint(config.vocoder_checkpoint_path)\n",
    "vocoder.load_state_dict(vocoder_state_dict[\"generator\"])\n",
    "vocoder.remove_weight_norm()\n",
    "vocoder.eval()\n",
    "model = FastSpeechLightning.load_from_checkpoint(\n",
    "    config.testing_checkpoint,\n",
    "    config=config,\n",
    "    vocoder=vocoder,\n",
    "    stft=stft,\n",
    "    train=False,\n",
    ")\n",
    "model.eval()\n",
    "torch.set_float32_matmul_precision(config.matmul_precision)\n",
    "# 遍历test_dataset\n",
    "# for i in range(len(test_dataset)):\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(len(test_dataset))):\n",
    "    audio_name, speaker_id, phone_sequence, emotion_id = test_dataset[i]\n",
    "    input_dict = get_input_dict(config, phone_sequence, speaker_id, emotion_id)\n",
    "    model_output = model.model(model.device, input_dict)\n",
    "    predicted_mel_len = model_output[\"mel_len\"][0]\n",
    "    predicted_mel_no_padding = model_output[\"predicted_mel\"][0, :predicted_mel_len]\n",
    "    generated_wav = synthesize_wav_from_mel(\n",
    "        predicted_mel_no_padding, model.vocoder, model.stft\n",
    "    )\n",
    "    write_wav(f'path/to/wav/save/path', generated_wav, config.sample_rate)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"run time: {elapsed_time} seconds.\")\n",
    "# input_dict = get_input_dict(config, phone_sequence, speaker_id, emotion_id)\n",
    "# model_output = model.model(model.device, input_dict)\n",
    "# predicted_mel_len = model_output[\"mel_len\"][0]\n",
    "# predicted_mel_no_padding = model_output[\"predicted_mel\"][0, :predicted_mel_len]\n",
    "# generated_wav = synthesize_wav_from_mel(\n",
    "#     predicted_mel_no_padding, model.vocoder, model.stft\n",
    "# )\n",
    "# write_wav(generated_audio_path, generated_wav, config.sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vits-Zvn1WcG2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
