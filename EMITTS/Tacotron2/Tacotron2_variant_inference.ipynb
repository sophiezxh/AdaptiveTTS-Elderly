{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tacotron 2 Variant inference code \n",
    "Edit the variables **checkpoint_path**, **text** and **emotion feature** to match yours and run the entire code to generate wav."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and setup matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hparams import create_hparams\n",
    "from model import Tacotron2\n",
    "from layers import TacotronSTFT, STFT\n",
    "from audio_processing import griffin_lim\n",
    "from train import load_model\n",
    "from text import text_to_sequence\n",
    "import sys\n",
    "sys.path.append(f'{os.path.join(\"/\", *os.getcwd().split(os.sep))}/waveglow')\n",
    "from denoiser import Denoiser\n",
    "from glow import WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, figsize=(16, 4)):\n",
    "    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
    "    fig.show()\n",
    "    for i in range(len(data)):\n",
    "        # masked_data = np.ma.masked_invalid(data[i])\n",
    "        axes[i].imshow(data[i], aspect='auto', origin='lower', \n",
    "                       interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup hparams && Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_speaker = True\n",
    "multi_emotion = True\n",
    "emotion_feature = True\n",
    "# emotion_feature = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams = create_hparams()\n",
    "# hparams = create_hparams(is_multi_speaker=False, is_multi_emotion=False, is_emotion_feature=False)\n",
    "# hparams = create_hparams(is_multi_speaker=True, is_multi_emotion=True, is_emotion_feature=False)\n",
    "hparams = create_hparams(is_multi_speaker=multi_speaker, is_multi_emotion=multi_emotion, is_emotion_feature=emotion_feature)\n",
    "hparams.sampling_rate = 22050\n",
    "\n",
    "if multi_speaker and multi_emotion and emotion_feature:\n",
    "    checkpoint_path = \"path/to/checkpoint\"\n",
    "elif multi_speaker and multi_emotion:\n",
    "    checkpoint_path = \"path/to/checkpoint\"\n",
    "\n",
    "model = load_model(hparams)\n",
    "model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load WaveGlow for mel2audio synthesis and denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveglow_path = 'path/to/waveglow_checkpoint'\n",
    "waveglow = torch.load(waveglow_path)['model']\n",
    "waveglow.cuda().eval()\n",
    "for k in waveglow.convinv:\n",
    "    k.float()\n",
    "denoiser = Denoiser(waveglow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"If they mother knew it.\"\n",
    "\n",
    "sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "sequence = torch.autograd.Variable(\n",
    "    torch.from_numpy(sequence)).cuda().long()\n",
    "\n",
    "# eid2efeature_name={\n",
    "#             0: 'neutral',\n",
    "#             1: 'angry',\n",
    "#             2: 'happy',\n",
    "#             3: 'sad',\n",
    "#             4: 'surprise',\n",
    "#         }\n",
    "\n",
    "# eid = 4\n",
    "# featurepath = f'path/to/mmefeature/{eid2efeature_name[eid]}.pt'\n",
    "# eid = torch.tensor([eid]).cuda().long()\n",
    "\n",
    "# sid, speaker id, e.g. 3\n",
    "sid = torch.tensor([3]).cuda().long()\n",
    "# featurepath, path to emotion feature, e.g. EPAlign/test/implict_fused/fused_feature_name.pt\n",
    "featurepath = 'path/to/emotion.pt'\n",
    "\n",
    "efeature = torch.load(featurepath).cuda().float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence, sid, efeature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode text input and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if multi_speaker and multi_emotion and emotion_feature:\n",
    "    mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence, speaker_id=sid, emotion_feature=efeature)\n",
    "elif multi_speaker and multi_emotion:\n",
    "    mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence, speaker_id=sid, emotion_id=eid)\n",
    "\n",
    "plot_data((mel_outputs.float().data.cpu().numpy()[0],\n",
    "           mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
    "           alignments.float().data.cpu().numpy()[0].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_outputs, mel_outputs_postnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthesize audio from spectrogram using WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    audio = waveglow.infer(mel_outputs_postnet, sigma=0.666)\n",
    "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# save wavform speech to file\n",
    "sf.write(\"speech.wav\", audio[0].data.cpu().numpy(), hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Remove WaveGlow bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_denoised = denoiser(audio, strength=0.01)[:, 0]\n",
    "ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import TextMelLoader\n",
    "from hparams import create_hparams\n",
    "\n",
    "# define above at Setup hparams && Load model from checkpoint section\n",
    "# multi_speaker = True\n",
    "# multi_emotion = True\n",
    "# emotion_feature = True\n",
    "\n",
    "training_files = 'path/to/filelists'\n",
    "validation_files = 'path/to/filelists'\n",
    "\n",
    "hparams = create_hparams(None, \n",
    "                        is_multi_speaker=True, \n",
    "                        is_multi_emotion=True, \n",
    "                        is_emotion_feature=True,\n",
    "                        training_files=training_files,\n",
    "                        validation_files=validation_files,)\n",
    "\n",
    "esd_en_dataset = TextMelLoader('path/to/filelists', hparams=hparams, is_return_path=True)\n",
    "# return >> text, mel, sid, eid, efeature, audio_path\n",
    "esd_en_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esd_en_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_speaker and multi_emotion and emotion_feature:\n",
    "    save_path = \"save/path\"\n",
    "elif multi_speaker and multi_emotion:\n",
    "    save_path = \"save/path\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "with torch.no_grad():\n",
    "    for i, items in tqdm(enumerate(esd_en_dataset)):\n",
    "        # print(i, items)\n",
    "        text, mel, sid, eid, efeature, audio_path = items\n",
    "        audio_name = audio_path.split('/')[-1]\n",
    "        text, sid, eid = text.cuda().long().unsqueeze(0), sid.cuda().long(), eid.cuda().long()\n",
    "        if emotion_feature:\n",
    "            efeature = efeature.cuda().float().unsqueeze(0)\n",
    "            mel_outputs, mel_outputs_postnet, _, alignments = model.inference(text, speaker_id=sid, emotion_id=eid, emotion_feature=efeature)\n",
    "        else:\n",
    "            mel_outputs, mel_outputs_postnet, _, alignments = model.inference(text, speaker_id=sid, emotion_id=eid)\n",
    "        audio = waveglow.infer(mel_outputs_postnet, sigma=0.666)\n",
    "        sf.write(f\"{save_path}/{audio_name}\", audio[0].data.cpu().numpy(), hparams.sampling_rate)\n",
    "        # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
